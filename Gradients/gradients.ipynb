{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To launch this notebook\n",
    "\n",
    "# python3 -m venv venv\n",
    "# .\\venv\\Scripts\\activate\n",
    "# code .\\gradients.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "%pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "It turns out that Deep Learning is about minimizing a loss function.\n",
    "\n",
    "Say we have a function that we want to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(x):\n",
    "    return x**2 - 6 * x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "We'd like to be able to get quickly to the minimum at x=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [ x / 10.0 for x in range(-20, 100)]\n",
    "ys = list(map(test_function, xs))\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "So we define a function that calculates the gradient and we can use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return 2 * x - 6\n",
    "\n",
    "x = 0\n",
    "path = [x]\n",
    "\n",
    "for _ in range(20):\n",
    "    x = x - 0.1 * grad(x)\n",
    "    path.append(x)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "And plot the stpes we took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ys)\n",
    "plt.plot(path, list(map(test_function, path)), \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "Taking gradients is hard, particularly when we have to use something like the chain rule.\n",
    "\n",
    "We can use TensorFlow's inbuilt gradient descent.\n",
    "\n",
    "We'll start at x=0 and take small steps in the direction where the function decreases.\n",
    "\n",
    "- This is obviously sensitive to how far we step, and we may step too far and never get closer.\n",
    " - We may also just find a local minimum as we aren't looking at the global view of the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "for _ in range(20):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    y = test_function(x)\n",
    "\n",
    "  dy_dx = tape.gradient(y, x)\n",
    "  x.assign_sub(alpha * dy_dx)\n",
    "\n",
    "  path.append(x.numpy())\n",
    "  \n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "And that took the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ys)\n",
    "plt.plot(path, list(map(test_function, path)), \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "And it is worth saying, we can use this technique in higher dimensions (and later we are going to have a lot of dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function2(x,y):\n",
    "    return (x - 1) ** 2 + (y - 1) ** 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "Which looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = y = np.arange(-3.0, 4.0, 0.05)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zs = np.array(test_function2(np.ravel(X), np.ravel(Y)))\n",
    "Z = zs.reshape(X.shape)\n",
    "\n",
    "ax.plot_surface(X, Y, Z)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "But it's just the same old code for minimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "\n",
    "x = tf.Variable(-3.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "for _ in range(100):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "   \n",
    "    z = test_function2(x,y)\n",
    "\n",
    "  dz_dx, dz_dy = tape.gradient(z, [x,y])\n",
    "  x.assign_sub(alpha * dz_dx)\n",
    "  y.assign_sub(alpha * dz_dy)\n",
    "\n",
    "  path.append((x.numpy(),y.numpy(),z))\n",
    "  \n",
    "print(x.numpy(),y.numpy())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "And these are the stpes we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for (x,y,z) in path:\n",
    "    ax.plot(x,y,z, \"o\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "There's lots we missed out there.\n",
    "- the choice of alpha and ideas like momentum\n",
    "- where we start\n",
    "- how we avoid just picking a local minimum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "We apply the same idea to train the neural network.\n",
    "\n",
    "We're going to build an AND gate. \n",
    "- TRUE is 1\n",
    "- FALSE is 0\n",
    "\n",
    "And the rules\n",
    "- TRUE AND TRUE = TRUE\n",
    "- FALSE AND TRUE = FALSE\n",
    "- TRUE AND FALSE = FALSE\n",
    "- FALSE AND FALSE = FALSE\n",
    "\n",
    "This is where we'll be using the chain rule as we have a neuron, an activation function and a loss function that we seek to minimize.\n",
    "\n",
    "First, we'll build and train the neuron by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(np.array([0.5,0.5]))\n",
    "bias = tf.Variable(0.3, dtype=\"float64\")\n",
    "\n",
    "def neuron(input):\n",
    "  n = tf.reduce_sum(tf.multiply(weights, input)) + bias\n",
    "  return tf.keras.activations.relu(n)\n",
    "\n",
    "for _ in range(100):\n",
    "  for datum in [[1., 1.], [0., 1.], [1., 0.], [0., 0.]]:\n",
    "    x = tf.constant(datum, dtype=\"float64\")\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      a = neuron(x)\n",
    "      expected = datum[0] * datum[1]\n",
    "      loss = (a - expected) ** 2\n",
    "\n",
    "    grad = tape.gradient(loss, [weights, bias])\n",
    "\n",
    "    for var in zip( [weights, bias], grad):\n",
    "      var[0].assign_sub(var[1] * 0.1)\n",
    "\n",
    "print (\"True, True, \", neuron(tf.constant([1., 1.], dtype=\"float64\")).numpy())\n",
    "print (\"False, True, \", neuron(tf.constant([0., 1.], dtype=\"float64\")).numpy())\n",
    "print (\"True, False, \", neuron(tf.constant([1., 0.], dtype=\"float64\")).numpy())\n",
    "print (\"False, False, \", neuron(tf.constant([0., 0.], dtype=\"float64\")).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "Use a [layer](https://keras.io/api/layers/), in particular a [Dense layer](https://keras.io/api/layers/core_layers/dense/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(1, activation='relu' )\n",
    "\n",
    "for _ in range(100):\n",
    "  for datum in [[1., 1.], [0., 1.], [1., 0.], [0., 0.]]:\n",
    "    x = tf.constant([datum])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      y = layer(x)\n",
    "      expected = datum[0] * datum[1]\n",
    "      loss = (y - expected)**2\n",
    "\n",
    "    grad = tape.gradient(loss, layer.trainable_variables)\n",
    "\n",
    "    for var in zip(layer.trainable_variables, grad):\n",
    "      var[0].assign_sub(var[1] * 0.1)\n",
    "\n",
    "print (\"True, True, \", layer(tf.constant([[1., 1.]])).numpy())\n",
    "print (\"False, True, \", layer(tf.constant([[0., 1.]])).numpy())\n",
    "print (\"True, False, \", layer(tf.constant([[1., 0.]])).numpy())\n",
    "print (\"False, False, \", layer(tf.constant([[0., 0.]])).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "And this is how you really do it, using a declarative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X = np.array([[1. ,1. ],[1., 0.],[0., 1.],[0., 0.]])\n",
    "y = np.array([[1.],[0.],[0.],[0.]])\n",
    "model.fit(X, y, epochs=1000, batch_size=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[1.,1.], [1.,0.],[0.,1.],[0.,0.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "So when we are in the context of the GradientTape the system is watching the forward calculation, and then using the associated functions in the backwards propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(100.)\n",
    "\n",
    "def log1pexp(x):\n",
    "  return tf.math.log(1 + tf.exp(x))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y=log1pexp(x)\n",
    "dy_dx = tape.gradient(y, x) \n",
    "\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(upstream):\n",
    "    return upstream * (1 - 1 / (1 + e))\n",
    "  return tf.math.log(1 + e), grad\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y=log1pexp(x)\n",
    "dy_dx = tape.gradient(y, x) \n",
    "\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "To the sources....\n",
    "\n",
    "[GradientTape is defined here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/backprop.py#L705).\n",
    "\n",
    "The [custom_gradient decorator is defined here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/custom_gradient.py#L47) with [the action defined here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/custom_gradient.py#L292), branching on [eager](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/custom_gradient.py#L536) and [graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/custom_gradient.py#L402) modes.\n",
    "\n",
    "In graph mode, we end up using [RegisterGradient](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/custom_gradient.py#L512)\n",
    "\n",
    "We register [a gradient handler here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L1405) and do so for all of the builtin ops.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "These are the handlers that are registered for the inbuilt ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops._gradient_registry._registry.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "In eager mode, we [record the gradient function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/record.py#L81) into the objects registered with the C++ kernel code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "traceback.print_stack()\n",
    "\n",
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(upstream):\n",
    "    traceback.print_stack()\n",
    "    return upstream * (1 - 1 / (1 + e))\n",
    "  return tf.math.log(1 + e), grad\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y=log1pexp(x)\n",
    "dy_dx = tape.gradient(y, x) \n",
    "\n",
    "print(dy_dx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
